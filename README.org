#+title: Deadlock in =yac_cget()=

This repository contains the code needed to reproduce a bug in YAC
version =3.0.2= (https://gitlab.dkrz.de/dkrz-sw/yac/) or possibly YAXT
version =0.10.0= (https://gitlab.dkrz.de/dkrz-sw/yaxt).

#+begin_quote
Depending on the parallel domain decomposition, =yac_cget()= may
deadlock in a =MPI_Waitall()= call.

This happens when using several MPI processes (in these test cases: 4)
to interpolate from a 900 m uniform Cartesian grid covering all of
Greenland to a 5 km grid using more-or-less the same domain.

I don't know how to reproduce this issue using 2 or 3 PEs: parallel
domain decomposition details matter.
#+end_quote

Grids mentioned above use projected coordinate systems and are
cumbersome to define in terms of longitude and latitude, so these test
cases use the =PROJ= library (version 6.0 or newer) to convert =(x,y)
-> (lon,lat)=.

The =Makefile= builds three executables: =test_0= and =test_1= use
different domain decompositions to reproduce the issue; =test_2= uses
a third domain decomposition to show that /some/ domain decompositions
are okay.

* Test case 0

The first test case (=test_0=) uses a domain decomposition similar to this:
#+begin_example
^
|
y
|
+----------+----------+
|          |          |
|          |          |
|  rank 2  |  rank 3  |
|          |          |
|          |          |
+----------+----------+
|          |          |
|          |          |
|  rank 0  |  rank 1  |
|          |          |
|          |          |
+----------+----------+-x->
#+end_example

* Test case 1

The second test case (=test_1=) uses a domain decomposition similar to
this:
#+begin_example
^
|
y
|
+----------------------+
|                      |
|                      |
+----------------------+
|                      |
|                      |
+----------------------+
|                      |
|                      |
+----------------------+
|                      |
|                      |
+----------------------+-x->
#+end_example

* Test case 2
The third test case (=test_2=) uses a domain decomposition similar to
this:
#+begin_example
^
|
y
|
+----+----+----+----+
|    |    |    |    |
|    |    |    |    |
|    |    |    |    |
|    |    |    |    |
|    |    |    |    |
|    |    |    |    |
|    |    |    |    |
|    |    |    |    |
|    |    |    |    |
|    |    |    |    |
|    |    |    |    |
|    |    |    |    |
+----+----+----+----+-x->
#+end_example

* Building

If =YAC=, =YAXT=, and =PROJ= are installed in standard locations, just
run =make=.
  
Otherwise edit =LIB_PREFIX= in =build.sh=, then run =./build.sh=.

* Running test cases

All three test cases succeed (and are identical since the domain
decomposition is the same) when executed using one MPI process. This
takes a while since the source grid contains over 5e6 points.

#+begin_src bash :results output :exports both :cache yes
/usr/bin/time ./test_0 2>&1
#+end_src

#+RESULTS[8bb49df1fe86982ba52e4b8862b9020b36b74a51]:
#+begin_example
Initializing the YAC instance... done
Defining components... done
Defining the source grid... done
Defining the target grid... done
Defining fields... done
Defining the interpolation stack... done
Defining the couple... done
Computing interpolation weights, etc... done
Calling yac_cput()... done
Calling yac_cget()... done
59.38user 6.76system 1:06.43elapsed 99%CPU (0avgtext+0avgdata 5786828maxresident)k
0inputs+0outputs (8major+5058422minor)pagefaults 0swaps
#+end_example

When running with 4 MPI processes, =mpiexec -n 4 test_0= and =mpiexec
-n 4 test_1= fail with an error message (see below) and =mpiexec -n 4 test_2=
succeeds, producing the same output (but faster).

#+begin_src bash :results output :exports both :cache yes
mpiexec -n 4 ./test_0 2>&1 || exit 0
#+end_src

#+RESULTS[05471f590a518c22346d70f198d4c7ab17446d49]:
#+begin_example
Initializing YAC... done
Defining components... done
Defining the source grid... done
Defining the target grid... done
Defining fields... done
Defining the interpolation stack... done
Defining the couple... done
Computing interpolation weights, etc... done
Calling yac_cput()... done
Calling yac_cget()... ERROR(yac_interpolation_direct_execute_get): state of exchange "source to target" is inconsistent 
Aborting in file interpolation_exchange.c, line 261 ...
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[64015,1],2]
  Exit code:    1
--------------------------------------------------------------------------
#+end_example

* Environment info

#+begin_src bash :results output :exports both :cache yes
lsb_release -d
ompi_info --version | head -1
mpicxx --version | head -1
echo PROJ `proj 2>&1 | head -1`
#+end_src

#+RESULTS[10b1006e52e05bcb8fa0259c14c1d35fd4550f38]:
: Description:	Pop!_OS 22.04 LTS
: Open MPI v4.1.2
: g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
: PROJ Rel. 8.2.1, January 1st, 2022

See =build-yac.sh= for the way =YAXT= 3.0.2 and =YAC= 0.10.0 were built.
